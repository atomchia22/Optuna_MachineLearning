{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpYhmmL3sZ2a"
      },
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import optuna\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---------- data ----------\n",
        "df = pd.read_csv('/content/drive/MyDrive/ML_22/data_all_2020.csv')\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['crop_types'])\n",
        "X = df.drop(columns=['crop_types']).select_dtypes(include=[np.number])\n",
        "\n",
        "# ---------- objective ----------\n",
        "def objective(trial, data=X, target=y):\n",
        "    Xtr, Xte, ytr, yte = train_test_split(\n",
        "        data, target, test_size=0.33, stratify=target, shuffle=True, random_state=42\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        'tree_method': 'hist',   # use 'gpu_hist' if your xgboost build supports CUDA\n",
        "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
        "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
        "        'colsample_bytree': trial.suggest_categorical('colsample_bytree',\n",
        "                                                      [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n",
        "        'subsample': trial.suggest_categorical('subsample',\n",
        "                                               [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
        "        'learning_rate': trial.suggest_categorical('learning_rate',\n",
        "                                                   [0.008,0.01,0.012,0.014,0.016,0.018,0.02]),\n",
        "        'max_depth': trial.suggest_categorical('max_depth',\n",
        "                                               [5,7,9,11,13,15,17]),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
        "        'n_estimators': 500,     # fixed since early stopping isnâ€™t available\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': int(np.unique(target).size),\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'n_jobs': -1,\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    model.fit(Xtr, ytr, eval_set=[(Xte, yte)], verbose=False)\n",
        "\n",
        "    preds = model.predict(Xte)\n",
        "    rmse = float(np.sqrt(mean_squared_error(yte, preds)))  # <-- no 'squared='\n",
        "    return rmse\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=3)\n",
        "\n",
        "print('Number of finished trials:', len(study.trials))\n",
        "print('Best trial params:', study.best_trial.params)\n",
        "print('Best RMSE:', study.best_trial.value)\n"
      ],
      "metadata": {
        "id": "G7Cr0pdfsgbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}